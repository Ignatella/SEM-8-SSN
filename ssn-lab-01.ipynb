{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:02<00:00, 81235878.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Definicja pierwszej warstwy konwolucyjnej: wejście 3 kanały (RGB), wyjście 32 kanały, rozmiar filtra 3x3, padding=1\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        # Definicja drugiej warstwy konwolucyjnej: wejście 32 kanały, wyjście 64 kanały, rozmiar filtra 3x3, padding=1\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        # Definicja trzeciej warstwy konwolucyjnej: wejście 64 kanały, wyjście 128 kanały, rozmiar filtra 3x3, padding=1\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        # Warstwa dropout1 z prawdopodobieństwem wyzerowania 25% neuronów\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        # Warstwa dropout2 z prawdopodobieństwem wyzerowania 50% neuronów\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Warstwa w pełni połączona (fully connected), wejście 128 * 4 * 4 neurony (wynik rozmiaru tensora po ostatniej konwolucji), wyjście 512 neurony\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        # Warstwa w pełni połączona (fully connected), wejście 512 neurony, wyjście 10 neurony (liczba klas)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Przekształcenie wejścia przez pierwszą warstwę konwolucyjną, zastosowanie funkcji aktywacji ReLU\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # Redukcja wymiarowości przez operację max-pooling z rozmiarem okna 2x2\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        # Przekształcenie przez drugą warstwę konwolucyjną, zastosowanie funkcji aktywacji ReLU\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # Redukcja wymiarowości przez operację max-pooling z rozmiarem okna 2x2\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        # Przekształcenie przez trzecią warstwę konwolucyjną, zastosowanie funkcji aktywacji ReLU\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        # Redukcja wymiarowości przez operację max-pooling z rozmiarem okna 2x2\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        # Wykorzystanie warstwy dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Spłaszczenie tensora do postaci wektora przed podaniem na warstwę w pełni połączoną\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Przekształcenie przez warstwę w pełni połączoną, zastosowanie funkcji aktywacji ReLU\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        # Wykorzystanie warstwy dropout2\n",
    "        x = self.dropout2(x)\n",
    "        # Przekształcenie przez warstwę w pełni połączoną (wyjściową)\n",
    "        x = self.fc2(x)\n",
    "        # Zastosowanie funkcji log_softmax dla uzyskania prawdopodobieństw przynależności do klas\n",
    "        output = nn.functional.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 2.280\n",
      "[1, 200] loss: 2.069\n",
      "[1, 300] loss: 1.943\n",
      "[1, 400] loss: 1.856\n",
      "[1, 500] loss: 1.750\n",
      "[1, 600] loss: 1.690\n",
      "[1, 700] loss: 1.642\n",
      "[2, 100] loss: 1.564\n",
      "[2, 200] loss: 1.528\n",
      "[2, 300] loss: 1.493\n",
      "[2, 400] loss: 1.476\n",
      "[2, 500] loss: 1.459\n",
      "[2, 600] loss: 1.435\n",
      "[2, 700] loss: 1.379\n",
      "[3, 100] loss: 1.335\n",
      "[3, 200] loss: 1.302\n",
      "[3, 300] loss: 1.270\n",
      "[3, 400] loss: 1.294\n",
      "[3, 500] loss: 1.246\n",
      "[3, 600] loss: 1.211\n",
      "[3, 700] loss: 1.221\n",
      "[4, 100] loss: 1.161\n",
      "[4, 200] loss: 1.147\n",
      "[4, 300] loss: 1.132\n",
      "[4, 400] loss: 1.162\n",
      "[4, 500] loss: 1.077\n",
      "[4, 600] loss: 1.086\n",
      "[4, 700] loss: 1.103\n",
      "[5, 100] loss: 1.041\n",
      "[5, 200] loss: 1.022\n",
      "[5, 300] loss: 0.999\n",
      "[5, 400] loss: 1.020\n",
      "[5, 500] loss: 1.031\n",
      "[5, 600] loss: 1.027\n",
      "[5, 700] loss: 0.991\n",
      "[6, 100] loss: 0.976\n",
      "[6, 200] loss: 0.950\n",
      "[6, 300] loss: 0.964\n",
      "[6, 400] loss: 0.954\n",
      "[6, 500] loss: 0.943\n",
      "[6, 600] loss: 0.949\n",
      "[6, 700] loss: 0.918\n",
      "[7, 100] loss: 0.947\n",
      "[7, 200] loss: 0.904\n",
      "[7, 300] loss: 0.897\n",
      "[7, 400] loss: 0.921\n",
      "[7, 500] loss: 0.912\n",
      "[7, 600] loss: 0.912\n",
      "[7, 700] loss: 0.869\n",
      "[8, 100] loss: 0.851\n",
      "[8, 200] loss: 0.864\n",
      "[8, 300] loss: 0.874\n",
      "[8, 400] loss: 0.855\n",
      "[8, 500] loss: 0.837\n",
      "[8, 600] loss: 0.848\n",
      "[8, 700] loss: 0.862\n",
      "[9, 100] loss: 0.823\n",
      "[9, 200] loss: 0.820\n",
      "[9, 300] loss: 0.805\n",
      "[9, 400] loss: 0.843\n",
      "[9, 500] loss: 0.822\n",
      "[9, 600] loss: 0.819\n",
      "[9, 700] loss: 0.817\n",
      "[10, 100] loss: 0.811\n",
      "[10, 200] loss: 0.795\n",
      "[10, 300] loss: 0.770\n",
      "[10, 400] loss: 0.755\n",
      "[10, 500] loss: 0.806\n",
      "[10, 600] loss: 0.794\n",
      "[10, 700] loss: 0.785\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 74.14%\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
