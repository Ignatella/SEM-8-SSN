{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFUH2mf8PTx5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sieci Neuronowe** (laboratorium 1/5)"
      ],
      "metadata": {
        "id": "Ra_AAv_XPcab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zasady zaliczenia\n",
        "1. **Obecność na laboratoriach:**\n",
        "   - Obecność na zajęciach laboratoryjnych jest obowiązkowa.\n",
        "   - Obecność jest potwierdzana poprzez wysłanie rozwiązania zadania przez UPEL.\n",
        "   - Każda nieobecnoś USPRAWIEDLIWIONA należy odrobić, najszybciej jak to jest możliwe. W przeciwnym wypadku student/studentka uzyskują 0pkt.\n",
        "   - Każda obecność nieusprawiedliwiona obniża ocenę końcową 1 stopień.\n",
        "\n",
        "2. **Odrabianie zajęć:**\n",
        "   - Zajęcia można odrobić w innej grupie laboratoryjnej, jeśli są dostępne miejsca.\n",
        "   - Pierwszeństwo w uczestnictwie mają osoby przypisane do danej grupy.\n",
        "   - Chęć odrabiania proszę zakomunikować przynajmniej dzień wcześniej.\n",
        "\n",
        "3. **Terminy odrabiania zajęć:**\n",
        "   - Odrabianie zajęć może odbywać się tylko w tygodniu, w którym dany temat jest analizowany.\n",
        "   - Student/Studentka zobowiązany jest zgłosić się do prowadzącego zajęcia w celu ustalenia terminu odrobienia zajęć.\n",
        "\n",
        "4. **Brak usprawiedliwionej nieobecności:**\n",
        "   - Student opuszczający więcej niż 50% zajęć bez usprawiedliwienia i uzyskujący negatywne wyniki w nauce może stracić możliwość wyrównania zaległości.\n",
        "   - Student może odwołać się od decyzji prowadzącego zajęcia do prowadzącego przedmiot i/lub Dziekana.\n",
        "\n",
        "6. **Usprawiedliwienia:**\n",
        "   - Usprawiedliwienia należy przesyłać w formie skanu na maila prowadzącego na adres: krzywda@agh.edu.pl, w temacie \"[SSN ##] Usprawiedliwienie\", gdzie ## to numer grupy.\n",
        "\n",
        "7. **Ocena laboratorium:**\n",
        "   - Ocena laboratorium będzie opierać się na ilości zdobytych punktów za sprawozdania. Sprawozdania wysyłacie Państwo na UPEL.\n",
        "   - Po każdych zajęciach należy przesłać sprawozdanie w formie notatnika .ipynb, w ramach którego umieszcza się komórki z kodem, wykresami oraz komórki z wnioskami,tekstem (markdown)\n",
        "   - Każde laboratorium ocenianie jest w skali 0-100. Żeby uzuskać zaliczenia należy uzyskać minimum **50%** punktów.\n",
        "   - Sprawozdań nie można poprawiać.\n",
        "   - Sprawozdanie można wysłać do dnia poprzedzającego kolejne zajęcia.\n",
        "   - Sprawozdania wysłane do tygodnia po terminie są oceniane w skali 0-50 (gdzie 50pkt to max za wykonanie wszystkich zadań). Po dwóch tygodniach max 25pkt itd.\n",
        "   - Ocena końcowa jest obliczana według zasad ogólnych AGH. Czyli:\n",
        "    - < 50.00% puktów to 2.0  \n",
        "    - 50.00-59.99% punktów to 3.0\n",
        "    - 60.00 -69.99% to 3.5\n",
        "    - 70.00 -79.99% to 4.0\n",
        "    - 80.00 - 89.99% to 4.5\n",
        "    - 90.00 % to 5.0\n",
        "    \n",
        "    Przykład:\n",
        "    Student/Studentka przesłała 3 sprawozdania, wszystkie idealne, na 100pkt oraz dwa przesyła tydzień po terminie to 2x50pkt. Wiec to łacznie 400pkt/500 ->\n",
        "\n",
        "8. **Tematyka zagadnień**\n",
        "    - Temat 1: Convolutional Neural Network cz. I\n",
        "    - Temat 2: Convolutional Neural Network cz. II\n",
        "    - Temat 3: Convolutional Neural Network cz. III + Recurrent Neural Network cz. I\n",
        "    - Temat 4: Recurrent Neural Network cz. II\n",
        "    - Temat 5: Graph Neural Networks\n",
        "\n",
        "9. **Technologie**\n",
        "    - PyTorch\n",
        "    - Google Colab\n"
      ],
      "metadata": {
        "id": "8KGjiGUcPi5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Wprowadzenie**"
      ],
      "metadata": {
        "id": "PcGNVSu7V5yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Omówienie kodu klasyfikacji obrazów z użyciem biblioteki PyTorch\n",
        "### Importowanie bibliotek\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "```\n",
        "\n",
        "Importowanie niezbędnych bibliotek: `torch` do obliczeń tensorowych, `torch.nn` do budowy sieci neuronowych, `torch.optim` do optymalizacji, a także moduł `torchvision` do pracy z zestawami danych obrazowych i transformacjami.\n",
        "\n"
      ],
      "metadata": {
        "id": "bW7pR01ObwHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie zestawu danych treningowych i testowych\n",
        "\n",
        "```python\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "```\n",
        "\n",
        "Przygotowanie transformacji dla zestawów treningowych i testowych, w tym losowe przycinanie, odbicia horyzontalne i normalizacja. Następnie pobranie zestawu danych CIFAR-10 i utworzenie obiektów DataLoader do ładowania danych treningowych i testowych partiami.\n",
        "\n"
      ],
      "metadata": {
        "id": "W6h_Z0zOb-O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definicja modelu sieci neuronowej\n",
        "\n",
        "```python\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Definicja pierwszej warstwy konwolucyjnej: wejście 3 kanały (RGB), wyjście 32 kanały, rozmiar filtra 3x3, padding=1\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        # Definicja drugiej warstwy konwolucyjnej: wejście 32 kanały, wyjście 64 kanały, rozmiar filtra 3x3, padding=1\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        # Definicja trzeciej warstwy konwolucyjnej: wejście 64 kanały, wyjście 128 kanały, rozmiar filtra 3x3, padding=1\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        # Warstwa dropout1 z prawdopodobieństwem wyzerowania 25% neuronów\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        # Warstwa dropout2 z prawdopodobieństwem wyzerowania 50% neuronów\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        # Warstwa w pełni połączona (fully connected), wejście 128 * 4 * 4 neurony (wynik rozmiaru tensora po ostatniej konwolucji), wyjście 512 neurony\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        # Warstwa w pełni połączona (fully connected), wejście 512 neurony, wyjście 10 neurony (liczba klas)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Przekształcenie wejścia przez pierwszą warstwę konwolucyjną, zastosowanie funkcji aktywacji ReLU\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        # Redukcja wymiarowości przez operację max-pooling z rozmiarem okna 2x2\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        # Przekształcenie przez drugą warstwę konwolucyjną, zastosowanie funkcji aktywacji ReLU\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        # Redukcja wymiarowości przez operację max-pooling z rozmiarem okna 2x2\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        # Przekształcenie przez trzecią warstwę konwolucyjną, zastosowanie funkcji aktywacji ReLU\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        # Redukcja wymiarowości przez operację max-pooling z rozmiarem okna 2x2\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        # Wykorzystanie warstwy dropout1\n",
        "        x = self.dropout1(x)\n",
        "        # Spłaszczenie tensora do postaci wektora przed podaniem na warstwę w pełni połączoną\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Przekształcenie przez warstwę w pełni połączoną, zastosowanie funkcji aktywacji ReLU\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        # Wykorzystanie warstwy dropout2\n",
        "        x = self.dropout2(x)\n",
        "        # Przekształcenie przez warstwę w pełni połączoną (wyjściową)\n",
        "        x = self.fc2(x)\n",
        "        # Zastosowanie funkcji log_softmax dla uzyskania prawdopodobieństw przynależności do klas\n",
        "        output = nn.functional.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "```\n",
        "\n",
        "Definicja klasy modelu sieci neuronowej. Warstwy konwolucyjne (Conv2d), warstwy Dropout (Dropout), oraz warstwy liniowe (Linear). Metoda forward definiuje przepływ danych przez sieć."
      ],
      "metadata": {
        "id": "NMZ4XxrhcHg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definicja funkcji straty i optymalizatora\n",
        "\n",
        "```python\n",
        "model = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "```\n",
        "\n",
        "Inicjalizacja modelu, funkcji straty (CrossEntropyLoss) oraz optymalizatora (SGD) z odpowiednimi parametrami.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UOxqcXSicSlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trenowanie modelu\n",
        "\n",
        "```python\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "```\n",
        "\n",
        "Pętla trenowania modelu przez 10 epok. Dla każdej epoki obliczane są straty na zestawie treningowym, propagowane wstecz i aktualizowane wagi sieci.\n",
        "\n"
      ],
      "metadata": {
        "id": "B6kKxHWicZKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testowanie modelu\n",
        "```python\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on the test set: {100 * correct / total:.2f}%\")\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "5fiY_9Lxce5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W **PyTorch** istnieje wiele przydatnych funkcji i warstw, które są wykorzystywane do budowy sieci neuronowych. Kilka z nich to:\n",
        "\n",
        "- [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html): Warstwa konwolucyjna dwuwymiarowa, używana do przetwarzania danych wizyjnych.\n",
        "\n",
        "- [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html): Warstwa dropout, która losowo zeruje część wejść w celu regularyzacji i zapobiegania przeuczeniu.\n",
        "\n",
        "- [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): Warstwa w pełni połączona, używana do transformacji liniowej danych.\n",
        "\n",
        "Ponadto, w module `torch.nn.functional` znajdują się różne funkcje aktywacji i operacje, takie jak:\n",
        "\n",
        "- [relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html): Funkcja aktywacji ReLU (Rectified Linear Unit), używana do wprowadzenia nieliniowości w sieciach neuronowych.\n",
        "\n",
        "- [max_pool2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html): Operacja max-pooling dla danych dwuwymiarowych, używana do redukcji wymiarowości.\n",
        "\n",
        "- [log_softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html): Funkcja aktywacji logarytmicznej softmax, używana do obliczania logarytmów prawdopodobieństw przynależności do klas.\n"
      ],
      "metadata": {
        "id": "ErRAPgawdL-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W PyTorch znajduje się wiele optymalizatorów, funkcji straty oraz funkcji aktywacji, które są powszechnie używane podczas trenowania sieci neuronowych. Kilka z nich to:\n",
        "\n",
        "Optymalizatory:\n",
        "\n",
        "- [SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD): Stochastyczny spadek gradientu, podstawowy optymalizator wykorzystywany do aktualizacji wag sieci.\n",
        "\n",
        "- [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam): Algorytm optymalizacji gradientowej stochastycznej, który adaptuje współczynniki uczenia dla każdego parametru.\n",
        "\n",
        "- [RMSprop](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop): Algorytm optymalizacji gradientowej stochastycznej, który korzysta z średniej ważonej kwadratów gradientów, aby adaptować współczynnik uczenia.\n",
        "\n",
        "Funkcje straty:\n",
        "\n",
        "- [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html): Funkcja straty krzyżowej entropii, często używana w zadaniach klasyfikacji wieloklasowej.\n",
        "\n",
        "- [MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html): Funkcja straty błędu średniokwadratowego, często używana w zadaniach regresji.\n",
        "\n",
        "Funkcje aktywacji:\n",
        "\n",
        "- [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html): Funkcja aktywacji ReLU (Rectified Linear Unit), wprowadza nieliniowość do modelu poprzez zastosowanie wartości maksymalnej z 0 i wartości x.\n",
        "\n",
        "- [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html): Funkcja aktywacji sigmoidalna, której wartości wyjściowe mieszczą się w zakresie (0, 1), często używana w ostatniej warstwie modelu do zadania binarnej klasyfikacji.\n",
        "\n",
        "- [Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html): Funkcja aktywacji tangens hiperboliczny, przekształca wartości wejściowe do zakresu (-1, 1).\n"
      ],
      "metadata": {
        "id": "zVizXh3IdeLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metryki** te są wykorzystywane do oceny jakości predykcji modelu podczas procesu treningu oraz testowania.\n",
        "\n",
        "1. Accuracy - metryka ta mierzy procent poprawnych klasyfikacji w zbiorze danych.\n",
        "\n",
        "2. Binary accuracy - metryka ta mierzy procent poprawnych klasyfikacji dla problemów binarnych.\n",
        "\n",
        "3. Categorical accuracy - metryka ta mierzy procent poprawnych klasyfikacji dla problemów wieloklasowych.\n",
        "\n",
        "4. Precision - metryka ta mierzy proporcję poprawnie sklasyfikowanych przykładów pozytywnych do ogólnej liczby sklasyfikowanych pozytywnie.\n",
        "\n",
        "5. Recall - metryka ta mierzy proporcję poprawnie sklasyfikowanych przykładów pozytywnych do ogólnej liczby prawdziwych pozytywnych.\n",
        "\n",
        "6. F1 score - metryka ta jest średnią harmoniczną precyzji i recall, a więc uwzględnia zarówno false positives, jak i false negatives.\n",
        "\n",
        "7. Mean squared error (MSE) - metryka ta mierzy średnią kwadratową różnicę między predykcjami a rzeczywistymi wartościami.\n",
        "\n",
        "8. Root mean squared error (RMSE) - metryka ta mierzy pierwiastek średniej kwadratowej różnicy między predykcjami a rzeczywistymi wartościami.\n",
        "\n",
        "9. Mean absolute error (MAE) - metryka ta mierzy średnią wartość bezwzględną różnicy między predykcjami a rzeczywistymi wartościami."
      ],
      "metadata": {
        "id": "5olDa5Ood2rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Warstwa konwolucyjna / splotowa (Convolutional Layers)\n",
        "\n",
        "Warstwa konwolucyjna, znana również jako warstwa splotowa, jest podstawowym elementem sieci neuronowych wykorzystywanych głównie w przetwarzaniu obrazów. Polega na przetwarzaniu sygnału wejściowego za pomocą filtrów konwolucyjnych, które przesuwane są po obrazie, generując mapy cech.\n",
        "\n",
        "`torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...)`\n",
        "\n",
        "- `in_channels`: Liczba kanałów wejściowych.\n",
        "- `out_channels`: Liczba kanałów wyjściowych (rozmiar głębokości mapy cech).\n",
        "- `kernel_size`: Rozmiar jądra (filtru).\n",
        "- `stride`: Krok przesunięcia filtra (domyślnie 1).\n",
        "- `padding`: Wypełnienie obrazu (domyślnie 0).\n",
        "- Inne parametry dostępne w warstwie konwolucyjnej w PyTorch.\n",
        "\n",
        "### Conv1D, Conv2D, Conv3D w PyTorch\n",
        "\n",
        "W PyTorch warstwy konwolucyjne dostępne są w różnych wymiarach przestrzennych:\n",
        "\n",
        "- **Conv1D**: Wykorzystywana głównie do przetwarzania szeregów czasowych.\n",
        "- **Conv2D**: Stosowana przede wszystkim w przetwarzaniu obrazów.\n",
        "- **Conv3D**: Używana w przypadku danych trójwymiarowych, np. wideo.\n",
        "\n",
        "### Warstwa poolingu\n",
        "\n",
        "Warstwa poolingu jest kolejnym elementem stosowanym w sieciach neuronowych, służącym do zmniejszania wymiarowości danych poprzez agregację informacji. Wyróżnia się kilka rodzajów warstw poolingu:\n",
        "\n",
        "- **Max Pooling**: Wybiera największą wartość z określonego obszaru.\n",
        "- **Average Pooling**: Oblicza średnią wartość z określonego obszaru.\n",
        "- **Global Pooling**: Agreguje informacje na poziomie całego obrazu lub wolumenu.\n",
        "- **Min Pooling**: Wybiera najmniejszą wartość z określonego obszaru.\n",
        "\n",
        "W PyTorch warstwy poolingu można zaimplementować za pomocą odpowiednich funkcji, np. `torch.nn.MaxPool2d`, `torch.nn.AvgPool2d`, itp.\n"
      ],
      "metadata": {
        "id": "MvxcRgMJehFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Normalization w PyTorch\n",
        "\n",
        "Batch Normalization (BN) to technika stosowana w sieciach neuronowych w celu poprawy szybkości uczenia się, stabilności i wydajności. W PyTorch Batch Normalization jest dostępna jako moduł `torch.nn.BatchNorm*d*`, gdzie `*d*` oznacza wymiar danych wejściowych (1D, 2D, itd.).\n",
        "\n",
        "#### Działanie Batch Normalization\n",
        "\n",
        "1. **Normalizacja Batch**: Dla każdej mini-batch danych, obliczane są średnia i odchylenie standardowe.\n",
        "2. **Normalizacja**: Dane wejściowe są normalizowane na podstawie obliczonych wartości średniej i odchylenia standardowego.\n",
        "3. **Skalowanie i Przesunięcie**: Dane są skalowane i przesuwane za pomocą dodatkowych parametrów w celu zachowania elastyczności modelu.\n",
        "\n",
        "#### Parametry Warstwy Batch Normalization\n",
        "\n",
        "- `num_features`: Liczba cech (kanałów) wejściowych.\n",
        "- `eps`: Parametr zapobiegający dzieleniu przez zero w przypadku niskiego odchylenia standardowego.\n",
        "- `momentum`: Parametr kontrolujący skalowanie wcześniejszej wartości średniej i odchylenia standardowego.\n",
        "\n",
        "#### Implementacja w PyTorch\n",
        "\n",
        "```python\n",
        "# Definicja modelu\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, num_features),\n",
        "    nn.BatchNorm1d(num_features),  # BatchNorm1d dla danych 1D\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(num_features, 1)\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZMnRCI9zfVJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularyzacja w PyTorch\n",
        "\n",
        "Regularyzacja jest techniką stosowaną w uczeniu maszynowym w celu zapobiegania nadmiernemu dopasowaniu (przeuczeniu) modelu do danych treningowych. W PyTorch można zaimplementować różne techniki regularizacji, takie jak L1, L2, Dropout itp.\n",
        "\n",
        "#### L1 i L2 Regularyzacja\n",
        "\n",
        "Regularyzacja L1 i L2 to popularne metody regularyzacji, które dodają kary do funkcji kosztu modelu na podstawie normy L1 lub L2 wag. W PyTorch regularyzacja L1 i L2 jest zazwyczaj implementowana poprzez dodanie kary do funkcji kosztu w trakcie treningu.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Przykładowa implementacja regularyzacji L2\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, output_size)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=0.001)  # weight_decay to parametr regularyzacji L2\n",
        "\n",
        "# W trakcie treningu\n",
        "for inputs, labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "```\n",
        "\n",
        "#### Dropout\n",
        "Dropout jest techniką regularizacji, która polega na losowym wyłączaniu neuronów w trakcie treningu, co zmusza model do bardziej równomiernego wykorzystania wszystkich cech. W PyTorch można zaimplementować warstwę Dropout za pomocą torch.nn.Dropout.\n",
        "\n",
        "```python\n",
        "# Przykładowa implementacja Dropout\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),  # p to prawdopodobieństwo wyłączenia neuronu\n",
        "    nn.Linear(hidden_size, output_size)\n",
        ")\n",
        "````\n",
        "\n",
        "Uwagi\n",
        "- Regularyzacja pomaga w zapobieganiu przeuczeniu modelu.\n",
        "- Odpowiedni dobór parametrów regularyzacji może poprawić ogólną zdolność generalizacji modelu.\n",
        "- Warto eksperymentować z różnymi technikami regularizacji i parametrami, aby uzyskać najlepsze rezultaty."
      ],
      "metadata": {
        "id": "CbIbfVTzf2ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Materiały do zapoznania się**\n",
        "https://poloclub.github.io/cnn-explainer/\n",
        "- https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
        "- https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/\n",
        "- https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
        "- https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/\n",
        "- https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
        "- http://yann.lecun.com/exdb/lenet/\n",
        "\n",
        "Best practices to build neural network models:\n",
        "\n",
        "* https://medium.com/@dipti.rohan.pawar/improving-performance-of-convolutional-neural-network-2ecfe0207de7\n",
        "\n",
        "* https://medium.com/towards-data-science/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b\n",
        "\n",
        "* https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d7\n",
        "\n",
        "\n",
        "\n",
        "Artykuły i snippety kodu + podstawowe zagadnienia do CNN\n",
        "\n",
        "* Idea konwolucji: http://sciagaprogramisty.blogspot.com/2018/01/konwolucja-wstep-do-neuronowych-sieci.html\n",
        "\n",
        "* NN architectures: https://pub.towardsai.net/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e?gi=28a8f0c56697\n",
        "\n",
        "* https://www.kaggle.com/code/cdeotte/how-to-choose-cnn-architecture-mnist/notebook\n",
        "\n",
        "* https://medium.com/analytics-vidhya/a-guide-to-neural-network-layers-with-applications-in-keras-40ccb7ebb57a\n",
        "\n",
        "* https://towardsdatascience.com/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4\n",
        "\n",
        "* https://medium.com/@andre_ye/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4\n",
        "\n",
        "* https://towardsdatascience.com/a-guide-to-neural-network-loss-functions-with-applications-in-keras-3a3baa9f71c5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Keras vs Tensorflow vs Pytorch\n",
        "\n",
        "\n",
        "\n",
        "* https://karliris62.medium.com/keras-vs-tensorflow-which-one-is-the-right-fit-for-your-project-7a166fe6c64b\n",
        "\n",
        "* https://www.edureka.co/blog/keras-vs-tensorflow-vs-pytorch/\n",
        "\n",
        "* https://towardsdatascience.com/batch-normalization-the-greatest-breakthrough-in-deep-learning-77e64909d81d\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dR6NlQRoewru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie 1"
      ],
      "metadata": {
        "id": "kcQFhk_6V-xG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Po zapoznaniu się z obsluga interfejsu Google Colab. **Zaimplementuj** trzy wielowarstwowe Convolutional Neural Networks (CNNs) do klasyfikacji datasetów:\n",
        "* [MNIST](https://paperswithcode.com/dataset/mnist)\n",
        "* [Fashion MNIST](https://paperswithcode.com/dataset/fashion-mnist)\n",
        "* [CIFAR-100](https://paperswithcode.com/sota/image-classification-on-cifar-100)\n",
        "\n",
        "W tym celu wykorzystaj biblioteke **PyTorch**.\n",
        "\n",
        "W sprawozdaniu odpowiedz na poniższe pytania opierając się o uzyskane wyniki w trakcie wykonywania eksperymentów.\n",
        "- Jakich warstw użyłeś/użyłaś w swoim modelu? Dlaczego? Dlaczego dobrałeś/dobrałaś takie parametry?\n",
        "- Narysuj przebieg funkcji straty oraz wybranych dwóch innych metryk (`matplotlib` albo wykorzystaj wbudowane funkcje w `PyTorch`). Co z nich wynika? Czym jest `overfitting` czym jest `underfitting`? Jaki mechanizm można zastosować żeby uniknać przeuczenia? Jak uniknąć niedouczenia?\n",
        "- Jakich optymalizatorów oraz funkcji straty użyłeś/aś? Dlaczego? Jakich parametrów? Za co odpowiada optymalizator w procesie uczenia?\n",
        "- Każdy model, dla każdego datasetu uruchom 5razy. Uśrednij wyniki i przedstaw to w formie tabelki(może być tabelka dataframe z `pandas` i wykorzystać `from tabulate import tabulate` do wyświetlenia tego w ładniejszy sposób).\n",
        "- Przygotuj macierz pomyłek dla każdego swojego modelu."
      ],
      "metadata": {
        "id": "6hIbLHF9WxLw"
      }
    }
  ]
}