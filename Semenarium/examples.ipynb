{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_PROFILE=dev\n"
     ]
    }
   ],
   "source": [
    "%env AWS_PROFILE dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-04 05:42:12 bgp-ls-demo\n",
      "2023-12-11 18:23:17 co-graph-drive-bucket-1\n",
      "2023-06-05 05:14:22 deployed-ignatella-app-pets\n",
      "2023-04-26 01:07:08 deployed-ignatella-tfstate\n",
      "2024-03-18 12:22:52 ignatella-ray\n",
      "2023-06-16 10:25:15 pt-ignatella-tfstate\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rekognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deployed.yermakovich.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cafe](./assets/cafe.jpg)\n",
    "![cafe](./assets/dog.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'Cafe',\n",
       "  'Confidence': 99.7676773071289,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Indoors'}, {'Name': 'Restaurant'}]},\n",
       " {'Name': 'Indoors',\n",
       "  'Confidence': 99.7676773071289,\n",
       "  'Instances': [],\n",
       "  'Parents': []},\n",
       " {'Name': 'Restaurant',\n",
       "  'Confidence': 99.7676773071289,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Indoors'}]},\n",
       " {'Name': 'Chair',\n",
       "  'Confidence': 97.89217376708984,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.1939052790403366,\n",
       "     'Height': 0.26556944847106934,\n",
       "     'Left': 0.802447497844696,\n",
       "     'Top': 0.7342319488525391},\n",
       "    'Confidence': 97.89217376708984},\n",
       "   {'BoundingBox': {'Width': 0.18260952830314636,\n",
       "     'Height': 0.2907703220844269,\n",
       "     'Left': 0.3120259642601013,\n",
       "     'Top': 0.7092296481132507},\n",
       "    'Confidence': 95.28466796875},\n",
       "   {'BoundingBox': {'Width': 0.14436249434947968,\n",
       "     'Height': 0.2813764810562134,\n",
       "     'Left': 0.5178850889205933,\n",
       "     'Top': 0.6781083941459656},\n",
       "    'Confidence': 75.41502380371094},\n",
       "   {'BoundingBox': {'Width': 0.12122442573308945,\n",
       "     'Height': 0.2400604784488678,\n",
       "     'Left': 0.4241557717323303,\n",
       "     'Top': 0.6456505060195923},\n",
       "    'Confidence': 68.31483459472656},\n",
       "   {'BoundingBox': {'Width': 0.17316699028015137,\n",
       "     'Height': 0.32222795486450195,\n",
       "     'Left': 0.6461644172668457,\n",
       "     'Top': 0.6775538921356201},\n",
       "    'Confidence': 66.15922546386719},\n",
       "   {'BoundingBox': {'Width': 0.12499549984931946,\n",
       "     'Height': 0.25670576095581055,\n",
       "     'Left': 0.22653834521770477,\n",
       "     'Top': 0.6654916405677795},\n",
       "    'Confidence': 56.87005615234375}],\n",
       "  'Parents': [{'Name': 'Furniture'}]},\n",
       " {'Name': 'Furniture',\n",
       "  'Confidence': 97.89217376708984,\n",
       "  'Instances': [],\n",
       "  'Parents': []},\n",
       " {'Name': 'Adult',\n",
       "  'Confidence': 95.0469741821289,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.1256113350391388,\n",
       "     'Height': 0.17567060887813568,\n",
       "     'Left': 0.8032882809638977,\n",
       "     'Top': 0.4902760684490204},\n",
       "    'Confidence': 95.0469741821289}],\n",
       "  'Parents': [{'Name': 'Person'}]},\n",
       " {'Name': 'Male',\n",
       "  'Confidence': 95.0469741821289,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.1256113350391388,\n",
       "     'Height': 0.17567060887813568,\n",
       "     'Left': 0.8032882809638977,\n",
       "     'Top': 0.4902760684490204},\n",
       "    'Confidence': 95.0469741821289}],\n",
       "  'Parents': [{'Name': 'Person'}]},\n",
       " {'Name': 'Man',\n",
       "  'Confidence': 95.0469741821289,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.1256113350391388,\n",
       "     'Height': 0.17567060887813568,\n",
       "     'Left': 0.8032882809638977,\n",
       "     'Top': 0.4902760684490204},\n",
       "    'Confidence': 95.0469741821289}],\n",
       "  'Parents': [{'Name': 'Adult'}, {'Name': 'Male'}, {'Name': 'Person'}]},\n",
       " {'Name': 'Person',\n",
       "  'Confidence': 95.0469741821289,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.1256113350391388,\n",
       "     'Height': 0.17567060887813568,\n",
       "     'Left': 0.8032882809638977,\n",
       "     'Top': 0.4902760684490204},\n",
       "    'Confidence': 95.0469741821289},\n",
       "   {'BoundingBox': {'Width': 0.056329406797885895,\n",
       "     'Height': 0.10329821705818176,\n",
       "     'Left': 0.42855942249298096,\n",
       "     'Top': 0.42237770557403564},\n",
       "    'Confidence': 91.93841552734375},\n",
       "   {'BoundingBox': {'Width': 0.025400977581739426,\n",
       "     'Height': 0.04483521729707718,\n",
       "     'Left': 0.034835267812013626,\n",
       "     'Top': 0.31627318263053894},\n",
       "    'Confidence': 61.19873046875}],\n",
       "  'Parents': []},\n",
       " {'Name': 'Plant',\n",
       "  'Confidence': 89.52864837646484,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.2387266308069229,\n",
       "     'Height': 0.643545389175415,\n",
       "     'Left': 0.5075492262840271,\n",
       "     'Top': 0.17103442549705505},\n",
       "    'Confidence': 89.52864837646484},\n",
       "   {'BoundingBox': {'Width': 0.03142894059419632,\n",
       "     'Height': 0.06379708647727966,\n",
       "     'Left': 0.08534056693315506,\n",
       "     'Top': 0.5151271224021912},\n",
       "    'Confidence': 58.13515853881836}],\n",
       "  'Parents': []},\n",
       " {'Name': 'Bar',\n",
       "  'Confidence': 75.84172058105469,\n",
       "  'Instances': [],\n",
       "  'Parents': []},\n",
       " {'Name': 'Head',\n",
       "  'Confidence': 66.6888198852539,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Person'}]},\n",
       " {'Name': 'Computer',\n",
       "  'Confidence': 65.4417495727539,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Electronics'}]},\n",
       " {'Name': 'Electronics',\n",
       "  'Confidence': 65.4417495727539,\n",
       "  'Instances': [],\n",
       "  'Parents': []},\n",
       " {'Name': 'Laptop',\n",
       "  'Confidence': 65.4417495727539,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.05056408792734146,\n",
       "     'Height': 0.045824144035577774,\n",
       "     'Left': 0.7629885077476501,\n",
       "     'Top': 0.5738970637321472},\n",
       "    'Confidence': 65.4417495727539}],\n",
       "  'Parents': [{'Name': 'Computer'}, {'Name': 'Electronics'}, {'Name': 'Pc'}]},\n",
       " {'Name': 'Pc',\n",
       "  'Confidence': 65.4417495727539,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Computer'}, {'Name': 'Electronics'}]},\n",
       " {'Name': 'Architecture',\n",
       "  'Confidence': 60.3677978515625,\n",
       "  'Instances': [],\n",
       "  'Parents': []},\n",
       " {'Name': 'Building',\n",
       "  'Confidence': 60.3677978515625,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.9977532625198364,\n",
       "     'Height': 1.0,\n",
       "     'Left': 0.002246784744784236,\n",
       "     'Top': 0.0},\n",
       "    'Confidence': 60.3677978515625}],\n",
       "  'Parents': [{'Name': 'Architecture'}]},\n",
       " {'Name': 'Face',\n",
       "  'Confidence': 57.73057174682617,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Head'}, {'Name': 'Person'}]},\n",
       " {'Name': 'Box',\n",
       "  'Confidence': 55.86455535888672,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.036579545587301254,\n",
       "     'Height': 0.03616310656070709,\n",
       "     'Left': 0.026812219992280006,\n",
       "     'Top': 0.5390317440032959},\n",
       "    'Confidence': 55.86455535888672}],\n",
       "  'Parents': []}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = boto3.client('rekognition', region_name=\"eu-west-1\")\n",
    "\n",
    "with open('./assets/cafe.jpg', 'rb') as img:\n",
    "    image_bytes = img.read()\n",
    "\n",
    "response = client.detect_labels(Image={'Bytes': image_bytes})\n",
    "\n",
    "response['Labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'Animal',\n",
       "  'Confidence': 98.85143280029297,\n",
       "  'Instances': [],\n",
       "  'Parents': []},\n",
       " {'Name': 'Canine',\n",
       "  'Confidence': 98.85143280029297,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Animal'}, {'Name': 'Mammal'}]},\n",
       " {'Name': 'Dog',\n",
       "  'Confidence': 98.85143280029297,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.9266027808189392,\n",
       "     'Height': 0.9226357936859131,\n",
       "     'Left': 0.006300697103142738,\n",
       "     'Top': 0.07573387026786804},\n",
       "    'Confidence': 98.85143280029297}],\n",
       "  'Parents': [{'Name': 'Animal'},\n",
       "   {'Name': 'Canine'},\n",
       "   {'Name': 'Mammal'},\n",
       "   {'Name': 'Pet'}]},\n",
       " {'Name': 'Mammal',\n",
       "  'Confidence': 98.85143280029297,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Animal'}]},\n",
       " {'Name': 'Pet',\n",
       "  'Confidence': 98.85143280029297,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Animal'}]},\n",
       " {'Name': 'Golden Retriever',\n",
       "  'Confidence': 88.34579467773438,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Animal'},\n",
       "   {'Name': 'Canine'},\n",
       "   {'Name': 'Dog'},\n",
       "   {'Name': 'Mammal'},\n",
       "   {'Name': 'Pet'}]},\n",
       " {'Name': 'Person',\n",
       "  'Confidence': 71.20465087890625,\n",
       "  'Instances': [{'BoundingBox': {'Width': 0.20558156073093414,\n",
       "     'Height': 0.8713437914848328,\n",
       "     'Left': 0.7925746440887451,\n",
       "     'Top': 0.07869529724121094},\n",
       "    'Confidence': 71.20465087890625}],\n",
       "  'Parents': []},\n",
       " {'Name': 'Labrador Retriever',\n",
       "  'Confidence': 65.28190612792969,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Animal'},\n",
       "   {'Name': 'Canine'},\n",
       "   {'Name': 'Dog'},\n",
       "   {'Name': 'Mammal'},\n",
       "   {'Name': 'Pet'}]},\n",
       " {'Name': 'Puppy',\n",
       "  'Confidence': 55.590091705322266,\n",
       "  'Instances': [],\n",
       "  'Parents': [{'Name': 'Animal'},\n",
       "   {'Name': 'Canine'},\n",
       "   {'Name': 'Dog'},\n",
       "   {'Name': 'Mammal'},\n",
       "   {'Name': 'Pet'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./assets/dog.jpg', 'rb') as img:\n",
    "    image_bytes = img.read()\n",
    "\n",
    "response = client.detect_labels(Image={'Bytes': image_bytes})\n",
    "\n",
    "response['Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wyklad](./assets/slide.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('textract', region_name=\"eu-west-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./assets/slide.jpg', \"rb\") as document_file:\n",
    "    document_bytes = document_file.read()\n",
    "\n",
    "response = client.analyze_document(\n",
    "    Document={\"Bytes\": document_bytes}, FeatureTypes=['TABLES','FORMS','SIGNATURES','LAYOUT']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SIMD',\n",
       " 'Te',\n",
       " 'same',\n",
       " 'operacje',\n",
       " 'wykonywane',\n",
       " 'jednoczesnie',\n",
       " 'na',\n",
       " 'róznych',\n",
       " 'danych.',\n",
       " 'Komputery',\n",
       " 'wektorowe',\n",
       " '-',\n",
       " 'jeden',\n",
       " 'procesor',\n",
       " 'operujacy',\n",
       " 'na',\n",
       " 'tablicy',\n",
       " 'danych.',\n",
       " 'WYJSCIE',\n",
       " 'EWAKUACYJNE',\n",
       " 'VYJSCIE',\n",
       " 'EWAKUACYJNE',\n",
       " 'Tablice',\n",
       " 'procesorów',\n",
       " '-',\n",
       " 'wiele',\n",
       " '(do',\n",
       " 'kilkunastu',\n",
       " 'tysiecy)',\n",
       " 'prostych',\n",
       " 'procesorów',\n",
       " 'wykonujacych',\n",
       " 'te',\n",
       " 'same',\n",
       " 'operacje',\n",
       " 'skalarne.',\n",
       " 'Drinz',\n",
       " 'Plote',\n",
       " 'Gronek',\n",
       " 'Wydzia',\n",
       " 'Fizyki',\n",
       " 'informatyki',\n",
       " 'Akademia',\n",
       " 'Gomizzo-Hutricza',\n",
       " 'Systemy',\n",
       " 'in',\n",
       " 'Stanislowa',\n",
       " 'Staszica',\n",
       " 'AGH',\n",
       " 'rownolegle',\n",
       " 'rozproszone']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b['Text'] for b in response['Blocks'] if b['BlockType'] == 'WORD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('transcribe', region_name=\"eu-north-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TranscriptionJob': {'TranscriptionJobName': 'voice2text-test',\n",
       "  'TranscriptionJobStatus': 'IN_PROGRESS',\n",
       "  'LanguageCode': 'pl-PL',\n",
       "  'MediaFormat': 'mp3',\n",
       "  'Media': {'MediaFileUri': 'https://bgp-ls-demo.s3.eu-north-1.amazonaws.com/voice-sample.mp3'},\n",
       "  'StartTime': datetime.datetime(2024, 3, 26, 10, 38, 41, 164000, tzinfo=tzlocal()),\n",
       "  'CreationTime': datetime.datetime(2024, 3, 26, 10, 38, 41, 120000, tzinfo=tzlocal())},\n",
       " 'ResponseMetadata': {'RequestId': 'c3f451bf-8f20-4a76-89a5-ba88ec973c4b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c3f451bf-8f20-4a76-89a5-ba88ec973c4b',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '297',\n",
       "   'date': 'Tue, 26 Mar 2024 09:38:40 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.start_transcription_job(\n",
    "    TranscriptionJobName='voice2text-test',\n",
    "    LanguageCode='pl-PL',\n",
    "    MediaFormat='mp3',\n",
    "    Media={\n",
    "        'MediaFileUri': 'https://bgp-ls-demo.s3.eu-north-1.amazonaws.com/voice-sample.mp3'\n",
    "    })\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TranscriptionJob': {'TranscriptionJobName': 'voice2text-test',\n",
       "  'TranscriptionJobStatus': 'COMPLETED',\n",
       "  'LanguageCode': 'pl-PL',\n",
       "  'MediaSampleRateHertz': 44100,\n",
       "  'MediaFormat': 'mp3',\n",
       "  'Media': {'MediaFileUri': 'https://bgp-ls-demo.s3.eu-north-1.amazonaws.com/voice-sample.mp3'},\n",
       "  'Transcript': {'TranscriptFileUri': 'https://s3.eu-north-1.amazonaws.com/aws-transcribe-eu-north-1-prod/422901541742/voice2text-test/7e66acb1-a4be-4146-a214-48bffd5bf16e/asrOutput.json?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCmV1LW5vcnRoLTEiRjBEAiBPUngHH11LtkB5encpw9nHY7skpbkpsImtnA6RuAZr2wIgObT60wJDEKfpr%2Bm%2FyRg80XV8Gk6venrn1Kk3JBqPvuEqxgUIov%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARABGgw4ODU4MjcxMDY1OTkiDM4od41iKeEkLmSgoSqaBf30zKYDxgZTRz2zRINngHGamyGNFpmEWORj1uKSSH7NCqGsocmKiy0aE1KQsdTI1KESpyPqrRZefCDiH%2BygoYPv08qYip4sIGSuym%2BVa1mKGPR5%2FMIF8SIWJGXNI%2BwiXxnXCvVtavpa%2F653Wjs5JqsRexbw0D4DV3QB3YVfWKcoENlizC7cnx%2BxhE9eRG5Ws7b3JSO2LLO04A%2B6%2FH6pk4R1JoyspuqK2t3abVJm%2Bdalv%2B6Ml3SacbAGBm08q8vBj%2BxRm5uoQ9Xt0Dw71vGDVYhetr8o1TvbUCn215QmEfLEfBLx1SnxrqJpWMI1sWZ7nphayaElAaVYm%2FU8Y5pe%2Fj7w2T7bYS%2FuoK9maC%2BKuCz6RxKLzZwgJtUR3%2FxLwa8VbdwBowIfr2dwOkRKq0Ryg3I%2FQ5x5crN4m5Ud2yTyeJfay%2FwBmjsbIdEtc8KKydEbwfE60a1%2FD3yaflBe9ZG%2FNEZCOop999BppGfIDKk%2Bd%2BmfFtDehJ7WaFx6%2BZdOPZLO6zpbDUxtOleFd3Zt51iN03IgmDj9yUGXe5iTl7fxC0tDM5QikPMO3kW8TupJDUJrsMnBgh2%2FlAuMM9myYLGPnuuCU%2FfGWG12arbYk5kRv%2FjCEWz2tzPsUOuKTyesvH9f7i3TCGs1i2CJag7%2BEwsNq0aE%2BA7ZKeB9tCjP2jpwboS5F7WPcWgAAWgSmWB%2BMtYCEsHqa3Motz7FYm6DMuKIVDfv3e6R929ieNQ7TE4N0r4ENYAsv1cu0w3ElHRQZEyH5PvvRbHZRyi2nlw1wt90KvUCQHkdJ%2BBvILphmsDKOuqQqtT4PqvDq6ydmD0Qg9zIGT2OuCXheUoMJJMme5u31FPWYepjdUVZid3i7aNviowNiI2wRgAwhBc%2FkjCOmYqwBjqyAS3MAH71N65t%2FFcTYGXPHaIry9cSK6JAnNbKp%2F5lb2kBLWpU0J8I5yXSf4qyan0fKLH20uly1Gjt%2BfsNokK2rUUYy5YDqaBXBtMYgsDRGqJjq9Epej%2BJc6bLEVQFULaVVJe%2FL2IH69hu8Pre2mEcmgN5TjVelXVT7lPFkwjDNfagn6kD6L0Akazvcf6GcoF0Zn0bSKDqU8FAySxVSi4RWvJ7MkN0CK5UYPvVVnYdl7VfXps%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240326T094024Z&X-Amz-SignedHeaders=host&X-Amz-Expires=900&X-Amz-Credential=ASIA44P3I64TWUWKBNXR%2F20240326%2Feu-north-1%2Fs3%2Faws4_request&X-Amz-Signature=6eae6d242af7209782284889d7cbb05a5edab7ee8664b3df6675c297c138290b'},\n",
       "  'StartTime': datetime.datetime(2024, 3, 26, 10, 38, 41, 164000, tzinfo=tzlocal()),\n",
       "  'CreationTime': datetime.datetime(2024, 3, 26, 10, 38, 41, 120000, tzinfo=tzlocal()),\n",
       "  'CompletionTime': datetime.datetime(2024, 3, 26, 10, 38, 47, 387000, tzinfo=tzlocal()),\n",
       "  'Settings': {'ChannelIdentification': False, 'ShowAlternatives': False}},\n",
       " 'ResponseMetadata': {'RequestId': 'c7582c11-0a13-41e0-8501-173e0ba89fd6',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c7582c11-0a13-41e0-8501-173e0ba89fd6',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2362',\n",
       "   'date': 'Tue, 26 Mar 2024 09:40:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.get_transcription_job(\n",
    "        TranscriptionJobName='voice2text-test',\n",
    "    )\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobName': 'voice2text-test',\n",
       " 'accountId': '422901541742',\n",
       " 'status': 'COMPLETED',\n",
       " 'results': {'transcripts': [{'transcript': 'gdybyśmy poszli wczoraj to jużbyśmy byli na miejscu'}],\n",
       "  'items': [{'type': 'pronunciation',\n",
       "    'alternatives': [{'confidence': '0.229', 'content': 'gdybyśmy'}],\n",
       "    'start_time': '0.4',\n",
       "    'end_time': '1.039'},\n",
       "   {'type': 'pronunciation',\n",
       "    'alternatives': [{'confidence': '0.992', 'content': 'poszli'}],\n",
       "    'start_time': '1.039',\n",
       "    'end_time': '1.399'},\n",
       "   {'type': 'pronunciation',\n",
       "    'alternatives': [{'confidence': '0.996', 'content': 'wczoraj'}],\n",
       "    'start_time': '1.399',\n",
       "    'end_time': '1.99'},\n",
       "   {'type': 'pronunciation',\n",
       "    'alternatives': [{'confidence': '0.996', 'content': 'to'}],\n",
       "    'start_time': '2.2',\n",
       "    'end_time': '2.359'},\n",
       "   {'type': 'pronunciation',\n",
       "    'alternatives': [{'confidence': '0.305', 'content': 'jużbyśmy'}],\n",
       "    'start_time': '2.359',\n",
       "    'end_time': '2.92'},\n",
       "   {'type': 'pronunciation',\n",
       "    'alternatives': [{'confidence': '0.99', 'content': 'byli'}],\n",
       "    'start_time': '2.92',\n",
       "    'end_time': '3.24'},\n",
       "   {'type': 'pronunciation',\n",
       "    'alternatives': [{'confidence': '0.997', 'content': 'na'}],\n",
       "    'start_time': '3.24',\n",
       "    'end_time': '3.359'},\n",
       "   {'type': 'pronunciation',\n",
       "    'alternatives': [{'confidence': '0.994', 'content': 'miejscu'}],\n",
       "    'start_time': '3.359',\n",
       "    'end_time': '3.98'}]}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./assets/transcribe-output.json', 'r') as f:\n",
    "    transcribe_output = json.load(f)\n",
    "\n",
    "json_str = json.dumps(transcribe_output, indent=4)\n",
    "\n",
    "# Print the pretty-printed JSON string\n",
    "eval(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code guru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://eu-north-1.console.aws.amazon.com/codeguru/reviewer/associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wyklad](./assets/code-guru-pending-review.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wyklad](./assets/code-guru-no-reccomendations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Memory leak](https://github.com/Ignatella/code-guru-test/pull/4/commits/bdff8fdcc991b230991b777f68b076b5fad72e2e)\n",
    "\n",
    "[Resource leak](https://github.com/Ignatella/code-guru-test/pull/3/commits/db826ba5cefcadd282870a06562a3d3459c24f4e)\n",
    "\n",
    "[Credential in public code](https://github.com/Ignatella/code-guru-test/pull/2/commits/ed46475feefae50f15ba20f40084ee249b635d18)\n",
    "\n",
    "[Overflow](https://github.com/Ignatella/code-guru-test/pull/1/commits/f2f3da94d69cb0ff23edf60322baf4eb94670f87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS Deep Composer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://us-east-1.console.aws.amazon.com/deepcomposer/home#modelDetail/genre-rock-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrz ray folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sage maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aws.amazon.com/sagemaker/pricing/\n",
    "\n",
    "https://aws.amazon.com/sagemaker/getting-started/\n",
    "\n",
    "https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://eu-north-1.console.aws.amazon.com/sagemaker/home?region=eu-north-1#/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aws.amazon.com/codewhisperer/?did=ft_card&trk=ft_card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wyklad](./assets/azure-ai-services.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Obtaining dependency information for python-dotenv from https://files.pythonhosted.org/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-cognitiveservices-vision-computervision\n",
      "  Obtaining dependency information for azure-cognitiveservices-vision-computervision from https://files.pythonhosted.org/packages/f4/cc/d371c24ef8e984c1dfde5c7837c4ab03dc3cdafc321d1f733518e5148567/azure_cognitiveservices_vision_computervision-0.9.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading azure_cognitiveservices_vision_computervision-0.9.0-py2.py3-none-any.whl.metadata (24 kB)\n",
      "Collecting msrest>=0.5.0 (from azure-cognitiveservices-vision-computervision)\n",
      "  Obtaining dependency information for msrest>=0.5.0 from https://files.pythonhosted.org/packages/15/cf/f2966a2638144491f8696c27320d5219f48a072715075d168b31d3237720/msrest-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting azure-common~=1.1 (from azure-cognitiveservices-vision-computervision)\n",
      "  Obtaining dependency information for azure-common~=1.1 from https://files.pythonhosted.org/packages/62/55/7f118b9c1b23ec15ca05d15a578d8207aa1706bc6f7c87218efffbbf875d/azure_common-1.1.28-py2.py3-none-any.whl.metadata\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting azure-core>=1.24.0 (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision)\n",
      "  Obtaining dependency information for azure-core>=1.24.0 from https://files.pythonhosted.org/packages/d7/70/180df3b43ebc7a1ec957d9e5c2c76e6c54398ec61a67dff88d3e0131be80/azure_core-1.30.1-py3-none-any.whl.metadata\n",
      "  Downloading azure_core-1.30.1-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ignatella/anaconda3/envs/MIO/lib/python3.10/site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2024.2.2)\n",
      "Collecting isodate>=0.6.0 (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision)\n",
      "  Obtaining dependency information for isodate>=0.6.0 from https://files.pythonhosted.org/packages/b6/85/7882d311924cbcfc70b1890780763e36ff0b140c7e51c110fc59a532f087/isodate-0.6.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /home/ignatella/anaconda3/envs/MIO/lib/python3.10/site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.3.0)\n",
      "Requirement already satisfied: requests~=2.16 in /home/ignatella/anaconda3/envs/MIO/lib/python3.10/site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.28.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/ignatella/anaconda3/envs/MIO/lib/python3.10/site-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.16.0)\n",
      "Collecting typing-extensions>=4.6.0 (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision)\n",
      "  Obtaining dependency information for typing-extensions>=4.6.0 from https://files.pythonhosted.org/packages/f9/de/dc04a3ea60b22624b51c703a84bbe0184abcd1d0b9bc8074b5d6b7ab90bb/typing_extensions-4.10.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ignatella/anaconda3/envs/MIO/lib/python3.10/site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ignatella/anaconda3/envs/MIO/lib/python3.10/site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ignatella/anaconda3/envs/MIO/lib/python3.10/site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (1.26.15)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ignatella/anaconda3/envs/MIO/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-computervision) (3.2.2)\n",
      "Downloading azure_cognitiveservices_vision_computervision-0.9.0-py2.py3-none-any.whl (39 kB)\n",
      "Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m738.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m757.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m902.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: azure-common, typing-extensions, isodate, azure-core, msrest, azure-cognitiveservices-vision-computervision\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "triton 2.0.0 requires cmake, which is not installed.\n",
      "triton 2.0.0 requires lit, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed azure-cognitiveservices-vision-computervision-0.9.0 azure-common-1.1.28 azure-core-1.30.1 isodate-0.6.1 msrest-0.7.1 typing-extensions-4.10.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-cognitiveservices-vision-computervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " %reload_ext dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Tag an image - remote =====\n",
      "Tags in the remote image: \n",
      "'outdoor' with confidence 99.00%\n",
      "'building' with confidence 98.81%\n",
      "'sky' with confidence 98.21%\n",
      "'stadium' with confidence 98.17%\n",
      "'ancient rome' with confidence 96.16%\n",
      "'ruins' with confidence 95.04%\n",
      "'amphitheatre' with confidence 93.99%\n",
      "'ancient roman architecture' with confidence 92.65%\n",
      "'historic site' with confidence 89.55%\n",
      "'ancient history' with confidence 89.54%\n",
      "'history' with confidence 86.72%\n",
      "'archaeological site' with confidence 84.41%\n",
      "'travel' with confidence 65.85%\n",
      "'large' with confidence 61.02%\n",
      "'city' with confidence 56.57%\n",
      "\n",
      "End of Computer Vision quickstart.\n"
     ]
    }
   ],
   "source": [
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "import os\n",
    "\n",
    "'''\n",
    "Authenticate\n",
    "Authenticates your credentials and creates a client.\n",
    "'''\n",
    "subscription_key = os.environ[\"VISION_KEY\"]\n",
    "endpoint = os.environ[\"VISION_ENDPOINT\"]\n",
    "\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
    "'''\n",
    "END - Authenticate\n",
    "'''\n",
    "\n",
    "'''\n",
    "Quickstart variables\n",
    "These variables are shared by several examples\n",
    "'''\n",
    "# Images used for the examples: Describe an image, Categorize an image, Tag an image, \n",
    "# Detect faces, Detect adult or racy content, Detect the color scheme, \n",
    "# Detect domain-specific content, Detect image types, Detect objects\n",
    "\n",
    "remote_image_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/master/ComputerVision/Images/landmark.jpg\"\n",
    "'''\n",
    "END - Quickstart variables\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Tag an Image - remote\n",
    "This example returns a tag (key word) for each thing in the image.\n",
    "'''\n",
    "print(\"===== Tag an image - remote =====\")\n",
    "# Call API with remote image\n",
    "tags_result_remote = computervision_client.tag_image(remote_image_url)\n",
    "\n",
    "# Print results with confidence score\n",
    "print(\"Tags in the remote image: \")\n",
    "if (len(tags_result_remote.tags) == 0):\n",
    "    print(\"No tags detected.\")\n",
    "else:\n",
    "    for tag in tags_result_remote.tags:\n",
    "        print(\"'{}' with confidence {:.2f}%\".format(tag.name, tag.confidence * 100))\n",
    "print()\n",
    "'''\n",
    "END - Tag an Image - remote\n",
    "'''\n",
    "print(\"End of Computer Vision quickstart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categories': [{'name': 'indoor_', 'score': 0.8828125}], 'color': {'dominantColorForeground': 'Grey', 'dominantColorBackground': 'Grey', 'dominantColors': ['Grey'], 'accentColor': '926839', 'isBwImg': False, 'isBWImg': False}, 'description': {'tags': ['text', 'indoor', 'chair', 'floor', 'ceiling', 'area'], 'captions': [{'text': 'a restaurant with tables and chairs', 'confidence': 0.4621441960334778}]}, 'requestId': 'f450268e-c7ea-40a9-ad33-3d0d83f41a1c', 'metadata': {'height': 686, 'width': 1000, 'format': 'Jpeg'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "analyze_url = endpoint + \"vision/v3.1/analyze\"\n",
    "\n",
    "image_data = open('./assets/cafe.jpg', \"rb\").read()\n",
    "headers = {'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "           'Content-Type': 'application/octet-stream'}\n",
    "params = {'visualFeatures': 'Categories,Description,Color'}\n",
    "response = requests.post(\n",
    "    analyze_url, headers=headers, params=params, data=image_data)\n",
    "response.raise_for_status()\n",
    "\n",
    "# The 'analysis' object contains various fields that describe the image. The most\n",
    "# relevant caption for the image is obtained from the 'description' property.\n",
    "analysis = response.json()\n",
    "image_caption = analysis[\"description\"][\"captions\"][0][\"text\"].capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A restaurant with tables and chairs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"categories\": [\n",
      "        {\n",
      "            \"name\": \"indoor_\",\n",
      "            \"score\": 0.8828125\n",
      "        }\n",
      "    ],\n",
      "    \"color\": {\n",
      "        \"dominantColorForeground\": \"Grey\",\n",
      "        \"dominantColorBackground\": \"Grey\",\n",
      "        \"dominantColors\": [\n",
      "            \"Grey\"\n",
      "        ],\n",
      "        \"accentColor\": \"926839\",\n",
      "        \"isBwImg\": false,\n",
      "        \"isBWImg\": false\n",
      "    },\n",
      "    \"description\": {\n",
      "        \"tags\": [\n",
      "            \"text\",\n",
      "            \"indoor\",\n",
      "            \"chair\",\n",
      "            \"floor\",\n",
      "            \"ceiling\",\n",
      "            \"area\"\n",
      "        ],\n",
      "        \"captions\": [\n",
      "            {\n",
      "                \"text\": \"a restaurant with tables and chairs\",\n",
      "                \"confidence\": 0.4621441960334778\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"requestId\": \"f450268e-c7ea-40a9-ad33-3d0d83f41a1c\",\n",
      "    \"metadata\": {\n",
      "        \"height\": 686,\n",
      "        \"width\": 1000,\n",
      "        \"format\": \"Jpeg\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_str = json.dumps(analysis, indent=4)\n",
    "\n",
    "# Print the pretty-printed JSON string\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure ML Studio\n",
    "\n",
    "https://ml.azure.com/?wsid=/subscriptions/d542226d-d16e-422f-9669-fb81c4884609/resourcegroups/test/providers/Microsoft.MachineLearningServices/workspaces/test&tid=80b1033f-21e0-4a82-bbc0-f05fdccd3bc8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/meta-llama/llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wyklad](./assets/azure-gpu-runtimes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wyklad](./assets/azure-no-gpu-response.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cloud.google.com/solutions/ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cloud.google.com/vision/docs/labels#vision_label_detection-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
